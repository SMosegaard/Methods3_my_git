---
title: "Assignment 2 - Meta-analysis of pitch in schizophrenia"
author: "Laura, Ida, Marie and Sofie"
date: "16/8/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

pacman::p_load(msm, tidyverse, brms, grid, gridExtra, readxl, metafor, dplyr, magrittr, reshape)
```

##### Assignment 2: meta-analysis #####

# Question 1


1. Simulate data to setup the analysis and gain insight on the structure of the problem. 
Simulate one dataset of 100 studies (n of participants should follow a normal distribution with mean of 20, sd of 10, but no fewer than 10 participants), with a mean effect size of 0.4, average deviation by study of .4 and measurement error of .8. 
The data you get should have one row per study, with an effect size mean and standard error. 

Build a proper bayesian model to analyze the simulated data. 

Then simulate publication bias (only some of the studies you simulate are likely to be published, which?), the effect of publication bias on your estimates (re-run the model on published studies, assess the difference), and discuss what this implies for your model. remember to use at least one plot to visualize your results. 

BONUS question: do a power/precision analysis: w this kind of sample sizes (participants) how many studies would you need to acquire good precision (e.g. .1 sd in the pop level estimate)

#Simulate data
```{r}
# Identify parameters

Pop_EffectMean <- 0.4 #The mean of the effect size for the true population, i.e. mu of the overall distribution the all the underlying distributions come from
Pop_StudySD <- 0.4 #The sd of that true effect size distribution
Error <- 0.8 # a standard error measurement, same for all i.e. it is a value, not a distribution

# number of studies
Studies <- 100

de <- tibble(
  Study = seq(Studies),
  Participants = round(msm::rtnorm(Studies, 20, 10, lower = 10), 0), 
  StudyEffect = NA, #Effect size drawn from the true distribution
  Sam_EffectMu = NA, #Effect size of that specific study (based on participants, StudyEffect and TSE)
  Sam_EffectSigma = NA, #Spread of the effect size for that specific study 
  TSE = 0.8, #True standard error, i.e. the spread of the StudyEffect estimate 
  Published = NA, #whether or not the study is published
)

#Start the simulation 
for(i in seq(Studies)){
  de$StudyEffect[i] <- rnorm(1, Pop_EffectMean, Pop_StudySD)
  sampling <- rnorm(de$Participants[i], de$StudyEffect[i], Error)
  de$Sam_EffectMu[i] <- mean(sampling)
  de$Sam_EffectSigma[i] <- sd(sampling)/sqrt(de$Participants[i])
}


```


#Build model with simulated data
#defining the formula
```{r}
# the model creates a distribution over our intercept (which is our mean)
# within each intercept we have the difference between the two groups (schizophrenia and non-schizophrenia)
#the model looks at each study (whom has a individual distribution, sd, error etc.)

#note that now effect size is a distribution, not a point-estimate! (the sign | indicates this)

#we tell it to weight by SE, the more error the less weight (so studies with more error should "have less to say" in the general pictures, as they are more uncertain and could drag us in the wrong direction --> so we do this to make a more precise model)


model_1 <- bf(Sam_EffectMu | se(Sam_EffectSigma) ~ 1 + (1|Study))


# | = the effect size is a distribution
# se(Sam_EffectSigma) = weigth by SE
# (1|Study)) = random effects

```

#Getting priors
```{r}
get_prior(data = de,
          family = gaussian,
          model_1)
```


#setting priors
```{r}

model_1_priors <- c(
prior(normal(0, 0.3), class = Intercept),
prior(normal(0, 0.3), class = sd) 
)

#The output of the function tells us that we need to specify a prior for the intercept (i.e. 'Intercept') 

```

#Running a model that only samples from the prior

```{r}
model_1_only_priors <- 
  brm(
    model_1,
    data = de,
    family = gaussian,
    prior = model_1_priors,
    sample_prior = "only",
    iter = 2000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
    backend ="cmdstanr",
    threads = threading(2),
    refresh=0,
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
```


#look at pp-checks
```{r}
pp_check(model_1_only_priors, ndraws = 100)
```


#Fitting a model that also samples from the simulated dataset
```{r}
fitted_model_1 <- 
  brm(
    model_1,
    data = de,
    family = gaussian,
    prior = model_1_priors,
    sample_prior = T,
    iter = 2000, 
    warmup = 1000,
    cores = 2,
    refresh=0,
    chains = 2,
    backend ="cmdstanr",
    threads = threading(2),
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
```


#Look at pp-checks of the fitted model
```{r}
pp_check(fitted_model_1, ndraws = 100)

#Much better!
```


#Look at prior posterier update checks 
```{r}
#Make prior posterier update checks 

variables(fitted_model_1)
Posterior_m <- as_draws_df(fitted_model_1)

#Plot the prior-posterior update plot for the intercept 
ppp1 <- ggplot(Posterior_m) +
  geom_density(aes(prior_Intercept), fill="chartreuse2", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="deeppink", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()+
  ggtitle("intercept for study effect size")+
      theme(plot.title = element_text(size = 6, face = "bold"))

#Plot the prior-posterior update plot for sigma:
ppp2 <- ggplot(Posterior_m) +
  geom_density(aes(prior_sd_Study), fill="chartreuse2", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="deeppink", color="black",alpha=0.6) + 
  xlab('sigma') +
  theme_classic()+
  ggtitle("sigma for study effect study")+
      theme(plot.title = element_text(size = 6, face = "bold"))

#Plot the prior-posterior update plot for sigma for intercept:
ppp3 <- ggplot(Posterior_m) +
  geom_density(aes(prior_sd_Study), fill="chartreuse2", color="black",alpha=0.6) +
  geom_density(aes(sd_Study__Intercept), fill="deeppink", color="black",alpha=0.6) + 
  xlab('sigma for intercept') +
  theme_classic()+
  ggtitle("sigma for intercept")+
      theme(plot.title = element_text(size = 6, face = "bold"))


grid.arrange(ppp1, ppp2, ppp3)


#green = prior
#pink = simulated data 

#plot 1: we told the model, that there was no difference between the effect sizes for intercepts --> however, the simulated data has a mean at around 0.4 and the priors at 0.0. --> difference between the intercepts is then 0.4 (which also can be seen in our parameter recovery)
#Plot 2: sigma (sd) can't be negative
```


#Parameter recovery of the fitted model
```{r}
print(fitted_model_1)
```


#Implementing a publication bias
```{r}
#Make a column for publication bias
for(i in seq(Studies)){
  de$Published[i] <- ifelse(
    abs(de$Sam_EffectMu[i]) - (2*de$Sam_EffectSigma[i]) > 0,  
    rbinom(1,1,.9), rbinom(1,1,.1))
}

#We only 'want' the studies with effect sizes that are more than 2 times the standard deviation of the mean, i.e. significant results. If the criteria of significance is met, there is a 90% possibility of being published, if not only 10%. 
#We however don't care about whether the significant effect size is positive or negative. This is what the abs() specifies. 
# If we had stated a hypothesis about the direction of the effect (positive or negative), we could have further added this. 
```


#Make a new data set that only include the published ones
```{r}

new_de <- de%>%
  filter(Published==1)

```


#Do the whole bayesian flow again with the new dataset
```{r}
#Fit a model with the new data set
fitted_model_2 <- 
  brm(
    model_1,
    data = new_de,
    family = gaussian,
    prior = model_1_priors,
    sample_prior = T,
    iter = 2000, 
    warmup = 1000,
    cores = 2,
    chains = 2,
     backend ="cmdstanr",
    threads = threading(2),
    refresh=0,
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))

```


#Do posterior predictive checks
```{r}

pp_check(fitted_model_2, ndraws = 100)


#now we can compare the two models: the first with all the studies and the second with the published studies
```


# Prior posterior update checks of the model
```{r}

variables(fitted_model_2)
Posterior_m_2 <- as_draws_df(fitted_model_2)

#Plot the prior-posterior update plot for the intercept 
ppp1_fitted2 <- ggplot(Posterior_m_2) +
  geom_density(aes(prior_Intercept), fill="chartreuse2", color="black",alpha=0.6) +
  geom_density(aes(b_Intercept), fill="deeppink", color="black",alpha=0.6) + 
  xlab('Intercept') +
  theme_classic()+
  ggtitle("intercept for study effect size")+
      theme(plot.title = element_text(size = 10, face = "bold"))

#Plot the prior-posterior update plot for sigma:
ppp2_fitted2 <- ggplot(Posterior_m_2) +
  geom_density(aes(prior_sd_Study), fill="chartreuse2", color="black",alpha=0.6) +
  geom_density(aes(sigma), fill="deeppink", color="black",alpha=0.6) + 
  xlab('sigma') +
  theme_classic()+
  ggtitle("sigma for study effect study")+
      theme(plot.title = element_text(size = 10, face = "bold"))

grid.arrange(ppp1_fitted2, ppp2_fitted2)


#Plot 1: we get a higher effect when we only look at the published studies --> becomes higher than our TES, because the studies included in the model of course has higher effect sizes
        #we want to get as close to the true effect size of the population

```


#Parameter recovery of the new model
```{r}
print(fitted_model_2)
```

#Visualize the difference of the models
```{r}

ggplot() +
  geom_density(aes(Posterior_m$b_Intercept), fill="cyan", color="cyan",alpha=0.6) +
  geom_density(aes(Posterior_m_2$b_Intercept), fill="blue1", color="blue1",alpha=0.6) + 
  xlab('Intercept') +
  theme_minimal()+
  ggtitle("Visualization of the two mean effectsizes")+
      theme(plot.title = element_text(size = 12, face = "bold"))

```


--------------------------------------

## Question 2

What is the current evidence for distinctive vocal patterns in schizophrenia? 
Use the data from Parola et al (2020) - https://www.dropbox.com/s/0l9ur0gaabr80a8/Matrix_MetaAnalysis_Diagnosis_updated290719.xlsx?dl=0 - focusing on pitch variability (PITCH_F0SD).  

Describe the data available (studies, participants). 
Using the model from question 1 analyze the data, visualize and report the findings: population level effect size; how well studies reflect it; influential studies, publication bias. 

BONUS question: assess the effect of task on the estimates (model comparison with baseline model)


#load data
```{r}

real_data <- read_excel("Matrix_MetaAnalysis_Diagnosis_updated290719.xlsx")

```


# Make dataset
```{r}

data_PITCH_F0SD <- real_data %>% 
  select(StudyID, ArticleID, contains("MALE_"), contains("FEMALE_"), contains("AGE_"), contains("SAMPLE_SIZE"),  contains("PITCH_F0SD_"))%>% 
  drop_na(contains("PITCH_F0SD_"))%>% 
  mutate(FEMALE_SZ = as.numeric(FEMALE_SZ)) %>% 
  mutate(FEMALE_HC = as.numeric(FEMALE_HC)) %>% 
  mutate(MALE_SZ = as.numeric(MALE_SZ)) %>% 
  mutate(MALE_HC = as.numeric(MALE_HC)) %>%
  mutate(AGE_M_HC = as.numeric(AGE_M_HC)) %>% 
  mutate(AGE_SD_HC = as.numeric(AGE_SD_HC)) %>% 
  mutate(AGE_M_SZ = as.numeric(AGE_M_SZ))%>%
  mutate(AGE_SD_SZ = as.numeric(AGE_SD_SZ))

data_PITCH_F0SD$names <- rownames(data_PITCH_F0SD)

data_PITCH_F0SD <- data_PITCH_F0SD%>%
  mutate(names = as.numeric(names))

```

## Calculate Effect sizes of pitch and SE (for the studies)
```{r}
# Find the effect size of every study containing the four parameters Mean, SD
# calculate Cohen's d
PitchMean <- escalc('SMD',
                    n1i =SAMPLE_SIZE_HC, n2i=SAMPLE_SIZE_SZ,
                    m1i = PITCH_F0SD_HC_M, m2i = PITCH_F0SD_SZ_M,
                    sd1i = PITCH_F0SD_HC_SD, sd2i = PITCH_F0SD_SZ_SD,
                    data = data_PITCH_F0SD)


#renaming the columns
PitchMean <-  PitchMean %>% 
  rename(Cohens_d = yi, Cohens_d_var = vi) 


## This shows that the healthy control group in most cases have more variability in pitch as they have a positive cohen's d. The ones with negative cohen's d shows that the healthy control group has less variability in pitch 


```


# Describe the data
```{r}

#Age (geom)

plot21 <- ggplot(PitchMean, aes(names)) +
  geom_point(aes(names, AGE_M_SZ), color = "seagreen") + # must include argument label "data"
  geom_point(aes(names, AGE_M_HC), color = "orchid2")+
  geom_errorbar(aes(ymin=AGE_M_SZ-AGE_SD_SZ, ymax=AGE_M_SZ+AGE_SD_SZ), color = "seagreen")+ 
  geom_errorbar(aes(ymin=AGE_M_HC-AGE_SD_HC, ymax=AGE_M_HC+AGE_SD_HC), color = "orchid2")+
    xlab('Study') +
  ylab('Age')+
  theme_minimal()+
  ggtitle(label=" green = HC, light pink = SZ ",subtitle = "Age by study")+
      theme(plot.title = element_text(size = 6, face = "bold"))

#Gender (density)


PitchMean[is.na(PitchMean)] <- 0
  
test_m <- PitchMean %>% 
  summarise(m_F_SZ = mean(FEMALE_SZ), m_F_HC = mean(FEMALE_HC), m_M_SZ = mean(MALE_SZ), m_M_HC = mean(MALE_HC))

  
plotdata=data.frame("Gender"=c("Female","Male"),
                    "HC"=c(15.26667,18.4),
                    "SZ"=c(11.86667,32.53333),
                    row.names = c("st1","st2"))

#Convert to long format
d = melt(plotdata, id.vars = "Gender") #install library "reshape"

plot22 <- ggplot(data = d,
       mapping = aes(x = Gender, y = value, fill = variable)) + 
    geom_col(position = position_dodge())


#Pitch (geom)

plot23 <- ggplot(PitchMean, aes(names)) +
  geom_point(aes(names, PITCH_F0SD_HC_M), color = "seagreen") + # must include argument label "data"
  geom_point(aes(names, PITCH_F0SD_SZ_M), color = "orchid2")+
  geom_errorbar(aes(ymin=PITCH_F0SD_HC_M - PITCH_F0SD_HC_SD, ymax = PITCH_F0SD_HC_M + PITCH_F0SD_HC_SD), color = "seagreen")+ 
  geom_errorbar(aes(ymin=PITCH_F0SD_SZ_M - PITCH_F0SD_SZ_SD, ymax=PITCH_F0SD_SZ_M + PITCH_F0SD_SZ_SD), color = "orchid2")+
    xlab('Study') +
  ylab('Pitch')+
  theme_minimal()+
  ggtitle(label="Pitch by study")

#Sample sizes (density)
plot24 <- ggplot() +
  geom_density(aes(PitchMean$SAMPLE_SIZE_HC), fill="seagreen", color="black",alpha=0.6) +
  geom_density(aes(PitchMean$SAMPLE_SIZE_SZ), fill="orchid2", color="black",alpha=0.6) + 
  xlab('Sample sizes') +
  theme_minimal()+
  ggtitle(label="Plot of distributions")

grid.arrange(plot21, plot22, plot23, plot24, nrow =  2)


```


### Make the model with the data
```{r}
model_1_p <- bf(Cohens_d | se(Cohens_d_var)  ~ 1 + (1|StudyID))
```

### Get priors
```{r}
get_prior(data = PitchMean,
          family = gaussian,
          model_1_p)
```


###setting priors
```{r}
model_1_p_priors <- c(
prior(normal(0, 0.3), class = Intercept),
prior(normal(0, 0.3), class = sd) 
)
```


### Fit the model
```{r}
fitted_model_1_p <- 
  brm(
    model_1_p,
    data = PitchMean,
    family = gaussian,
    prior = model_1_p_priors,
    sample_prior = T,
    iter = 2000, 
    warmup = 1000,
    cores = 2,
    backend ="cmdstanr",
    threads = threading(2),
    refresh=0,
    chains = 2,
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
```


###look at pp-check
```{r}
pp_check(fitted_model_1_p, ndraws = 100)
```


### Parameter recovery
```{r}
print(fitted_model_1_p)
```


### Visualize findings: population level effect size and cohens d with error bars 
```{r}
ggplot(PitchMean, aes(names)) +
  geom_point(aes(names, Cohens_d), color = "black") + 
  geom_errorbar(aes(ymin=Cohens_d - Cohens_d_var, ymax = Cohens_d + Cohens_d_var), color = "black")+
  geom_hline(yintercept = c(.1), color = "red")
  

#the red line is the estimate from our parameter recovery --> the small points shows the different models and their variance --> the bigger variance will say less (as we weight our model and want it as precise as possible)

```


### Create new dataframe where publication bias from Q1 is applied 
```{r}
for(i in seq(PitchMean$StudyID)){
  PitchMean$Published[i] <- ifelse(
    abs(PitchMean$Cohens_d[i]) - (2*PitchMean$Cohens_d_var[i]) > 0,  
    rbinom(1,1,.9), rbinom(1,1,.1))
}

new_pitch <- PitchMean%>%
  filter(Published==1)
```



### Fit the model with new data 
```{r}
fitted_model_1_p_new <- 
  brm(
    model_1_p,
    data = new_pitch,
    family = gaussian,
    prior = model_1_p_priors,
    sample_prior = T,
    iter = 2000, 
    warmup = 1000,
    cores = 2,
    backend ="cmdstanr",
    threads = threading(2),
    refresh=0,
    chains = 2,
    control = list(
      adapt_delta = 0.999,
      max_treedepth = 20))
```


### Parameter recovery
```{r}
print(fitted_model_1_p_new)

```


### Visualize findings: population level effect size and cohens d per study with error bars 
```{r}

ggplot(new_pitch, aes(names)) +
  geom_point(aes(names, Cohens_d), color = "black") + 
  geom_errorbar(aes(ymin=Cohens_d - Cohens_d_var, ymax = Cohens_d + Cohens_d_var), color = "black", width = 0.4)+
  geom_hline(yintercept = c(.15), color = "red")

```


